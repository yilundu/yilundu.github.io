<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation">
    <meta name="author"
          content="Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B. Tenenbaum, Alberto Rodriguez, Pulkit Agrawal, Vincent Sitzmann">

    <title>Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Neural Descriptor Fields: <br> SE(3)-Equivariant Object Representations for Manipulation</h2>
    <h3>ICRA 2022</h3>
    <hr>
    <p class="authors">
        <a href="https://anthonysimeonov.github.io/">Anthony Simeonov<sup>*</sup></a>,
        <a href="https://yilundu.github.io">Yilun Du<sup>*</sup></a>,
        <a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,<br>
        <a href="https://mitibmwatsonailab.mit.edu/people/joshua-tenenbaum/"> Joshua B. Tenenbaum</a>,
        <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
        <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal<sup>+</sup></a>,
        <a href="https://vsitzmann.github.io/">Vincent Sitzmann<sup>+</sup></a>
    </p>
    <p class="authors">
      <sup>*</sup> Author contributed equally, order determined by coin flip.&nbsp;&nbsp;&nbsp;
      <sup>+</sup> Equal Advising
    </p>

    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2112.05124">Paper</a>
        <a class="btn btn-primary" href="https://github.com/anthonysimeonov/ndf_robot">Code</a>
        <a class="btn btn-primary" href="https://colab.research.google.com/drive/16bFIFq_E8mnAVwZ_V2qQiKp4x4D0n1sG?usp=sharing">Colab</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="vcontainer">
            <iframe class='video' src="https://www.youtube.com/embed/dXl9xI2LrRw" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>
        <hr>
        <p>
            We present Neural Descriptor Fields (NDFs), an object representation that encodes both points and relative poses between an object and a target (such as a robot gripper or a rack used for hanging) via category-level descriptors.

            We employ this representation for object manipulation, where given a task demonstration, we want to repeat the same task on a <em>new</em> object instance from the <em>same</em> category.

            We propose to achieve this objective by searching (via optimization) for the pose whose descriptor matches that observed in the demonstration.

            NDFs are conveniently trained in a self-supervised fashion via a 3D auto-encoding task that does not rely on expert-labeled keypoints.

            Further, NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across all possible 3D object translations <em>and</em> rotations.

            We demonstrate learning of manipulation tasks from few (~5-10) demonstrations both in simulation and on a real robot.

            Our performance generalizes across both object instances and 6-DoF object poses, and significantly outperforms a recent baseline that relies on 2D descriptors.
        </p>

    </div>


    <div class="section">
        <h2>Results</h2>
        <hr>
        <p>
            As humans, we are able to naturally and easily manipulate a wide variety of different objects from a single demonstration, regardless of the underlying pose or instance of the object. Inspired by this capability, in this paper,
            we present a framework which can <b>provably</b> generalize a single manipulation demonstration of an object to an <b>arbitrary novel</b> pose of the object. Given a manipulation skill executed a upright mug, we may automatically 
            apply this demonstration to the mug when it is oriented either sideways or even upside-down. We further show that our framework enables us to transfer demonstrations on different mugs to novel instances of a mug.
        </p>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/ndf_trimmed.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
    </div>

        <div class="section">
            <h2>Paper</h2>
            <hr>
            <div>
                <div class="list-group">
                    <a href=""
                       class="list-group-item">
                        <img src="img/paper_thumbnail.png"
                             style="width:100%; margin-right:-20px; margin-top:-10px;">
                    </a>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Bibtex</h2>
            <hr>
            <div class="bibtexsection">
                @article{simeonovdu2021ndf,
                  title={Neural Descriptor Fields: SE(3)-Equivariant Object 
                    Representations for Manipulation},
                  author={Simeonov, Anthony and Du, Yilun and Tagliasacchi, 
                        Andrea and Tenenbaum, Joshua B. and Rodriguez, Alberto and Agrawal, 
                        Pulkit and Sitzmann, Vincent},
                  booktitle = {ICRA},
                  year={2022}
                }
            </div>
        </div>

        <div class="section">
            <h2>Our Related Projects</h2>
            <hr>
            <p>
                Check out our related projects on neural fields and object representations for manipulation! <br>
            </p>

		<div class='row vspace-top'>
		    <div class="col-sm-3">
			<img src='img/multistep_taller.png' class='img-fluid'>
		    </div>

		    <div class="col">
			<div class='paper-title'>
			    <a href="https://arxiv.org/abs/2011.08177">A Long Horizon Planning Framework for Manipulating Rigid Pointcloud Objects</a>
			</div>
			<div>
                We present a framework for solving long-horizon planning problems involving manipulation of rigid objects  that operates directly from a point-cloud observation. Our method plans in the space of object subgoals and frees the planner from reasoning about robot-object interaction dynamics by relying on a set of generalizable manipulation primitives.  We show that for rigid bodies, this abstraction can be realized using low-level manipulation skills that maintain sticking contact with the object and represent subgoals as 3D transformations.
			</div>
		    </div>
		</div>
		
            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='img/audiovisual_manifold.gif' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://yilundu.github.io/gem/">Learning Signal-Agnostic Manifolds of Neural Fields</a>

                    </div>
                    <div>
                        We present a method to capture the underlying structure of arbitrary data signals by representing each point of data as a neural field. This enables us to interpolate and generate new samples in image, shape, audio, and audiovisual domains all using the same identical architecture.
                    </div>
                </div>
            </div>

            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='../nerflow/fig/gibson_full.gif' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://yilundu.github.io/nerflow/">Neural Radiance Flow for 4D View Synthesis and Video Processing</a>

                    </div>
                    <div>
                        We present a method to capture a dynamic scene utilizing a spatial-temporal radiance field. We enforce consistency in this field utilizing a continuous flow field. We show that such an approach enables us to synthesize novel views in dynamic scenes captured using as little as a single monocular video, and further show that our radiance field can be utilized to denoise and super-resolve input images.
                    </div>
                </div>
            </div>

            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='img/srn_seg_repimage.jpeg' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://www.computationalimaging.org/publications/semantic-srn/">Inferring Semantic Information with 3D Neural Scene Representations</a>
                    </div>
                    <div>
                        We demonstrate that the features learned by neural implicit scene representations are useful for downstream tasks, such as semantic segmentation, and propose a model that can learn to perform continuous 3D semantic segmentation on a class of objects (such as chairs) given only a single, 2D (!) semantic label map!
                    </div>
                </div>
            </div>


        <div class="section">
            <h2>External Related Projects</h2>
            <hr>
            <p>
                Check out these relevant external projects on utilizing object representations for manipulation! <br>
            </p>

		<div class='row vspace-top'>
		    <div class="col-sm-3">
			<img src='img/hats_trim.gif' class='img-fluid'>
		    </div>

		    <div class="col">
			<div class='paper-title'>
			    <a href="https://arxiv.org/abs/1806.08756">Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation</a>
			</div>
			<div>
                A method for self-supervised dense correspondence learning. Dense descriptors encoding correspondence are used as an object representation for manipulation. The authors demonstrate grasping of specific points on an object across potentially deformed object configurations, and using class-general descriptors to transfer specific grasps across different objects in a class.
			</div>
		    </div>
		</div>
		
            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='img/kpam.png' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://sites.google.com/view/kpam">KeyPoint Affordances for Category-Level Robotic Manipulation</a>

                    </div>
                    <div>
                        A formulation of category-level manipulation that uses semantic 3D keypoints as the object representation. This keypoint representation enables a simple and interpretable specification of the manipulation target as geometric costs and constraints on the keypoints, and is robust to large intra-category shape variation.
                    </div>
                </div>
            </div>


        <hr>

        <footer>
            <p>Send feedback and questions to <a href="https://anthonysimeonov.github.io/">Anthony Simeonov</a> and  <a href="https://yilundu.github.io">Yilun Du</a></p>
        </footer>
    </div>


    

</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
