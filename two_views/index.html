<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Learning to Render Novel Views from Wide-Baseline Stereo Pairs">

    <title>Learning to Render Novel Views from Wide-Baseline Stereo Pairs</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Learning to Render Novel Views from<br>Wide-Baseline Stereo Pairs</h2>
    <hr>
    <p class="authors">
        <a href="https://yilundu.github.io">Yilun Du</a>,
        <a href="">Cameron Smith</a>,
        <a href="https://ayushtewari.com/">Ayush Tewari<sup>+</sup></a>,
        <a href="https://vsitzmann.github.io/">Vincent Sitzmann<sup>+</sup></a>
    </p>
    <p class="authors">
      <sup>+</sup> Equal Advising
    </p>

    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="">Paper</a>
        <a class="btn btn-primary" href="">Code (Coming Soon!)</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <p>
            We introduce a method for novel view synthesis given only a single wide-baseline stereo image pair.
            In this challenging regime, 3D scene points are regularly observed only once, requiring prior-based reconstruction of scene geometry and appearance.

            We find that existing approaches to novel view synthesis from sparse observations fail due to recovering incorrect 3D geometry and due to the high cost of differentiable rendering that precludes their scaling to large-scale training.

            We take a step towards resolving these shortcomings by formulating a multi-view transformer encoder, proposing an efficient, image-space epipolar line sampling scheme to assemble image features for a target ray, and a lightweight cross-attention-based renderer.

            Our contributions enable training of our method on a large-scale real-world dataset of indoor and outdoor scenes.

            We demonstrate that our method learns powerful multi-view geometry priors while reducing both rendering time and memory footprint.

            We conduct extensive comparisons on held-out test scenes across two real-world datasets, significantly outperforming prior work on novel view synthesis from sparse image observations and achieving multi-view-consistent novel view synthesis.
        </p>
    </div>

    <div class="section">
        <h2>Results</h2>
        <hr>
        <p>
            Below, we illustrate novel view rendering results of our approach from different wide-baseline image
            pairs. Our approach is able to consistently render different wide baseline novel views.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/Main_5.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
    </div>

    <div class="section">
        <h2>Indoor Scene Baseline Comparisons</h2>
        <hr>
        <p>
            Next, we illustrate comparisons of our approach with IBRNet, pixelNeRF, GPNR on different indoor scene in Realestate10k.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/Comparisons_2.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
    </div>

    <div class="section">
        <h2>Outdoor Scene Baseline Comparisons</h2>
        <hr>
        <p>
            Next, we illustrate comparisons of our approach with IBRNet, pixelNeRF, GPNR on different outdoor scenes in ACID.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/acid_comparison.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
    </div>

    <div class="section">
        <h2>Novel View Synthesis from Unposed Images</h2>
        <hr>
        <p>
           Finally, we illustrate how approach can render novel views from images we obtained from the internet without relative pose.
        </p>
    </div>
    <div class="row align-items-center">
        <div class="col justify-content-center text-center">
            <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/Unposed_5.mp4" type="video/mp4">
            </video>
        </div> 
    </div>

    <div class="section">
        <h2>Limitations</h2>
        <hr>
        <p>
           In this paper, we've presented a new approach to render novel views from wide-baseline stereo
           pairs. However, this is a very challenging problem, and their are test scenes in which our approach 
           either fails to adequately estimate
           the depth of the scene or fails to obtain consistent multiview renderings of these scenes. We believe 
           both problems opens directions for future work.
        </p>
    </div>
    <div class="row align-items-center">
        <div class="col justify-content-center text-center">
            <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/Unposed_5.mp4" type="video/mp4">
            </video>
        </div> 
    </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href=""
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

        <div class="section">
            <h2>Our Related Projects</h2>
            <hr>
            <p>
                Check out our related projects on neural rendering and neural fields! <br>
            </p>


            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='../nerflow/fig/gibson_full.gif' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://yilundu.github.io/nerflow/">Neural Radiance Flow for 4D View Synthesis and Video Processing</a>

                    </div>
                    <div>
                        We present a method to capture a dynamic scene utilizing a spatial-temporal radiance field. We enforce consistency in this field utilizing a continuous flow field. We show that such an approach enables us to synthesize novel views in dynamic scenes captured using as little as a single monocular video, and further show that our radiance field can be utilized to denoise and super-resolve input images.
                    </div>
                </div>
            </div>

		<div class='row vspace-top'>
		    <div class="col-sm-3">
			<img src='../data/projects/ndf.gif' class='img-fluid'>
		    </div>

		    <div class="col">
			<div class='paper-title'>
			    <a href="https://arxiv.org/abs/2112.05124">Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation</a>
			</div>
			<div>
                We present Neural Descriptor Fields (NDFs), an SE3 equivariant object representation which enables manipulation of different categories of objects at arbitrary poses from a limited number (5-10) demonstrations.
            </div>
		    </div>
		</div>

            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='../ndf/img/audiovisual_manifold.gif' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://yilundu.github.io/gem/">Learning Signal-Agnostic Manifolds of Neural Fields</a>

                    </div>
                    <div>
                        We present a method to capture the underlying structure of arbitrary data signals by representing each point of data as a neural field. This enables us to interpolate and generate new samples in image, shape, audio, and audiovisual domains all using the same identical architecture.
                    </div>
                </div>
            </div>
    <hr>

</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
