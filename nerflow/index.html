<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Neural Radiance Flow for 4D View Synthesis and Video Processing">
    <meta name="author"
          content="Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, Jiajun Wu">

    <title>Neural Radiance Flow for 4D View Synthesis and Video Processing</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Neural Radiance Flow for 4D View Synthesis and Video Processing</h2>
    <h3>ICCV 2021</h3>
    <hr>
    <p class="authors">
        <a href="https://yilundu.github.io">Yilun Du<sup>1</sup></a>,
        <a href="">Yinan Zhang<sup>2</sup></a>,
        <a href="https://kovenyu.com/">Hong-Xing Yu<sup>2</sup></a>,
        <a href="https://mitibmwatsonailab.mit.edu/people/joshua-tenenbaum/">Joshua B. Tenenbaum<sup>1</sup></a>,
        <a href="https://jiajunwu.com/">Jiajun Wu<sup>2</sup></a>
    </p>
    <p class="institution">
      <sup>1</sup> MIT CSAIL&nbsp;&nbsp;&nbsp;
      <sup>2</sup> Stanford University
    </p>

    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2012.09790">Paper</a>
        <a class="btn btn-primary" href="https://github.com/yilundu/nerflow">Code</a>
    </div>
</div>


<div class="container">
    <div class="section">
        <p>
    We present a method, Neural Radiance Flow (NeRFlow), to learn a 4D spatial-temporal representation of a dynamic scene from a set of RGB images. Key to our approach is the use of a neural implicit representation that learns to  capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing consistency across different modalities, our representation enables multi-view rendering in diverse dynamic scenes, including water pouring, robotic interaction, and real images, outperforming state-of-the-art methods for spatial-temporal view synthesis. Our approach works even when inputs images are captured with only two separate cameras. We further demonstrate that the learned representation can serve an implicit scene prior, enabling video processing tasks such as image super-resolution and de-noising without any additional supervision.
        </p>
    </div>

    <div class="section">
        <h2>Full View Synthesis</h2>
        <hr>
        <p>
            We present NeRFLow, which learns a 4D spatial-temporal representation of a dynamic scene. In a scene with water pouring,  we are able to render novel images (left), infer the depth map (middle left), the underlying continuous flow field (middle right), and denoise input observations (right).
		</p>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src='fig/pouring_full.gif' class='img-fluid'>
            </div>
        </div>

        </br>

        <p>
            Our approach also works with synthesis results on the iGibson robot below, with rendered images on the left and inferred depth map in the middle. We are further able to synthesize dynamic animations (right).
		</p>

        <div class="row justify-content-left">
            <div class="col-sm-8">
                <img src="fig/gibson_full.gif" class="img-fluid">
            </div>
            <div class="col-sm-4">
                <img src="fig/dynamic_human.gif" class="img-fluid">
            </div>
         </div>

    </div>


    <div class="section">
        <h2>Sparse View Synthesis</h2>
        <hr>
        <p>
            Our approach is able to represent a dynamic scene when given only a limited number of views across the scene, as well as a sparse set of timestamps from which the underlying scene is captured with. We illustrate the ability to do novel view synthesis utilizing a dynamic scene captured from a single real monocular video. </p>

        <div class="row justify-content-left">
            <div class="col-sm-4">
                <img src="fig/ayush.gif" class="img-fluid" style="width:90%">
            </div>
            <div class="col-sm-2">
                <img src="fig/person_real.gif" class="img-fluid">
            </div>
            <div class="col-sm-6">
                <img src="fig/kid_real.gif" class="img-fluid" style="width:90%">
            </div>
         </div>

        </br>

        <p> 
        Next, we assess the ability of NeRFlow to synthesize scenes which are captured by a sparse set of timestamps. We capture the pouring animation utilizing only 1 in 10 frames of animation.  We animate the resultant pouring animation across frames of animation and find that NeRFlow with consistency (left) performs significantly better than a approach without consistency (right).
        </p>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src='fig/joint.gif' class='img-fluid'>
            </div>
        </div>


    </div>

    <div class="section">
        <h2>Video Processing</h2>
        <hr>
        <p>
        NeRFlow is able to capture and aggregate radiance information across different viewpoints and timesteps. By rendering from this aggregate representation, NeRFlow can serve as a scene prior for video processing tasks. We present results where we show that when NeRFlow is fit on noisy input images, renders from NeRFlow are able to denoise and super-resolve the input images.
        </p>

        <div class="row justify-content-left">
            <div class="col-sm-4">
                <img src="fig/denoise.gif" class="img-fluid">
            </div>
            <div class="col-sm-4">
                <img src="fig/superres.gif" class="img-fluid">
            </div>
            <div class="col-sm-3">
                <img src="fig/denoise_real.gif" class="img-fluid" style="width:95%">
            </div>
         </div>



    </div>

            <div class="section">
                <h2>Paper</h2>
                <hr>
                <div>
                    <div class="list-group">
                        <a href="https://arxiv.org/abs/2012.09790"
                           class="list-group-item">
                            <img src="fig/paper_thumbnail.png"
                                 style="width:100%; margin-right:-20px; margin-top:-10px;">
                        </a>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>Bibtex</h2>
                <hr>
                <div class="bibtexsection">
                    @inproceedings{du2021nerflow,
                      author    = {Yilun Du and Yinan Zhang and Hong-Xing Yu 
                                   and Joshua B. Tenenbaum and Jiajun Wu},
                      title     = {Neural Radiance Flow for 4D View Synthesis and Video Processing},
                      year      = {2021},
                      booktitle   = {Proceedings of the IEEE/CVF International Conference
                                     on Computer Vision},
                    }
                </div>
            </div>

            <hr>

            <footer>
                <p>Send feedback and questions to <a href="https://yilundu.github.io">Yilun Du</a></p>
            </footer>
        </div>

</body>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>
