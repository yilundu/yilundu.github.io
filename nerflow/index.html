<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en"><head>
  <title>Neural Radiance Flow for 4D View Synthesis and Video Processing</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="fig/uncond_gen.gif"/>
<meta property="og:title" content="Neural Radiance Flow for 4D View Synthesis and Video Processing" />
<meta http-equiv=“Pragma” content=”no-cache”>
<meta http-equiv=“Expires” content=”-1″>
<meta http-equiv=“CACHE-CONTROL” content=”NO-CACHE”>

<script src="lib.js" type="text/javascript"></script>
<script src="popup.js" type="text/javascript"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=1000,height=1000,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
    PADDING-RIGHT: 0px;
    PADDING-LEFT: 0px;
    FLOAT: right;
    PADDING-BOTTOM: 0px;
    PADDING-TOP: 0px
}
#primarycontent {
    MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
BODY {
    TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="b5m.js" id="b5mmain" type="text/javascript"></script></head>

<body>

<div id="primarycontent">
<center><h1>Neural Radiance Flow for 4D View Synthesis and Video Processing</h1></center>
<center><h2><a href="https://yilundu.github.io/">Yilun Du<sup>1</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="">Yinan Zhang<sup>2</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="https://kovenyu.com/">Hong-Xing Yu<sup>2</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum<sup>1</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="https://jiajunwu.com/">Jiajun Wu<sup>2</sup></a></h2></center>
<center><h2>
  <sup>1</sup> MIT CSAIL&nbsp;&nbsp;&nbsp;
  <sup>2</sup> Stanford University
</h2></center>
<center><h2><strong><a href="">Paper</a> | <a href="">Code (Coming Soon)</a></strong> </h2></center>
<br>


<!---
<center><a>
<img src="fig/comp_cartoon.jpg" width="1000"> 
</a></center>
<br>
-->



<p>
<h2>Abstract</h2>

<div style="font-size:14px"><p align="justify">
    We present a method, Neural Radiance Flow (NeRFlow), to learn a 4D spatial-temporal representation of a dynamic scene from a set of RGB images. Key to our approach is the use of a neural implicit representation that learns to  capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing consistency across different modalities, our representation enables multi-view rendering in diverse dynamic scenes, including water pouring, robotic interaction, and real images, outperforming state-of-the-art methods for spatial-temporal view synthesis. Our approach works even when inputs images are captured with only two separate cameras. We further demonstrate that the learned representation can serve an implicit scene prior, enabling video processing tasks such as image super-resolution and de-noising without any additional supervision.
</div>
</p>

<!-- ---------------------------------------------------------------------------------- -->
<a href=""><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="fig/paper_thumbnail.png" width=170></a>
<br>


<h2>Paper</h2>
<p><a href="">arxiv</a>,  2020. </p>


<h2>Citation</h2>
<p>Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, Jiajun Wu. "Neural Radiance Flow for 4D View Synthesis and Video Processing", arxiv.
<a href="">Bibtex</a>
</p>


<h2><a href=''>Code (Coming Soon)</a> </h2>

<br><br>

<!-- ---------------------------------------------------------------------------------- -->


<h2>Full View Synthesis</h2>

<p style="font-size:14px">
We present NeRFLow, which learns a 4D spatial-temporal representation of a dynamic scene. In a scene with water pouring,  we are able to render novel images (left), infer the depth map (middle left), the underlying continuous flow field (middle right), and denoise input observations (right).
</p>

<p>
<center><a> <img src="fig/pouring_full.gif" style="float:none;width:920px"> </a></center>
</p>


<p style="font-size:14px">
Our approach also works with synthesis results on the Gibson robot below, with rendered images on the left and inferred depth map on the right
</p>

<p>
<center><a> <img src="fig/gibson_full.gif" style="float:none;width:640px"> </a></center>
</p>


<h2>Sparse View Synthesis</h2>

<p style="font-size:14px">
Our approach is able to represent a dynamic scene when given  a limited number of views across the scene.
</p>

<h3>Stereo Cameras </h3>
<p style="font-size:14px">
 We consider a dynamic scene captured by two stereo cameras moving over a cup pouring liquid illustrated below: 
</p>
<p>
<center><a> <img src="fig/stereo_input.gif" style="float:none;width:640px"> </a></center>
</p>

<p style="font-size:14px">
In such a set up, we are able to render the entire dynamic animation from a fixed viewpoint (illustrated below), even though images are captured at opposite viewpoints over time.
</p>
<p>
<center><a> <img src="fig/stereo_render.gif" style="float:none;width:640px"> </a></center>
</p>

<h3>Opposite Cameras </h3>

<p style="font-size:14px">
 We further consider the Gibson scene captured by two opposite cameras moving (illustrated below) as a robot travels.
</p>

<p>
<center><a> <img src="fig/dual_input.gif" style="float:none;width:640px"> </a></center>
</p>

<p style="font-size:14px">
In such a set up, we are also able to render the entire dynamic animation from a fixed viewpoint (illustrated below), even though cameras move across different viewpoints over time
</p>

<p>
<center><a> <img src="fig/dual_render.gif" style="float:none;width:640px"> </a></center>
</p>



<h2>Temporal Interpolation</h2>

<p style="font-size:14px">
Our spatial-temporal representation allows us to interpolate and render intermediate timesteps inside a video. NeRFlow, with consistency, can generate a smooth rendering of pouring when trained on only 1 in 10 frames of the animation.  We compare NeRFlow approach with consistency (left) compared with an approach without consistency (right) 
</p>

<p>
<center><a> <img src="fig/joint.gif" style="float:none;width:640px"> </a></center>
</p>

<h2>Monocular Video</h2>

<p style="font-size:14px">
Our approach also allows us to synthesize real dynamic scenes captured by a monocular camera.
<p>

<p>
<center><a> <img src="fig/ayush.gif" style="float:none;width:360px"> </a></center>
</p>



<h2>Video Processing</h2>
<p style="font-size:14px">
NeRFlow is able to capture and aggregate radiance information across different viewpoints and timesteps. By rendering from this aggregate representation, NeRFlow can serve as a scene prior for video processing tasks. We present results where we show that when NeRFlow is fit on noisy input images, renders from NeRFlow are able to denoise and super-resolve the input images.
<p>

<center><a> <img src="fig/denoise.gif" style="float:none;width:350px"> <img src="fig/superres.gif" style="float:none;width:350px"> <img src="fig/denoise_real.gif" style="float:none;width:250px"> </a></center>
</p>
