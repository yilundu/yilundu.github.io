<!DOCTYPE html>
<html>
    <head>
        <title>Deep Reinforcement Learning on Space Invaders Using Keras –  – </title>

            <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="

" />
    <meta property="og:description" content="

" />
    
    <meta name="author" content="" />

    
    <meta property="og:title" content="Deep Reinforcement Learning on Space Invaders Using Keras" />
    <meta property="twitter:title" content="Deep Reinforcement Learning on Space Invaders Using Keras" />
    


        <!--[if lt IE 9]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->

        <link rel="stylesheet" type="text/css" href="/style.css" />
        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link rel="alternate" type="application/rss+xml" title=" - " href="/feed.xml" />
        <script type="text/javascript" async
                src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

        <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
    </head>

    <body>
        <div class="wrapper-masthead">
            <div class="container">
                <header class="masthead clearfix">
                    <a href="/" class="site-avatar"><img src="" /></a>

                    <div class="site-info">
                        <h1 class="site-name"><a href="/"></a></h1>
                        <p class="site-description"></p>
                    </div>

                    <nav>
                        <a href="/blog">Blog</a>
                        <a href="/">About</a>
                    </nav>
                </header>
            </div>
        </div>

        <div id="main" role="main" class="container">
            <article class="post">
  <h1>Deep Reinforcement Learning on Space Invaders Using Keras</h1>

  <div class="entry">
    <p><img src="http://localhost:4000/images/output.gif" alt="Picture space invaders" class="img-responsive center" /></p>

<p><a href="https://github.com/yilundu/DQN-DDQN-on-Space-Invaders">Full code for training Double Deep <script type="math/tex">Q</script> Network and Duel <script type="math/tex">Q</script> Network</a></p>

<p>Over the winter break I thought it would be fun to experiment with deep reinforcement learning. Using the ideas of reinforcement learning computers have been able to do amazing things such master the game of <a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf">Go</a>, play 3D racing games <a href="https://arxiv.org/abs/1602.01783">competitively</a>, and undergo complex manipulations of the environment around them that completely defy explicit programming!</p>

<p>A little under 3 years ago, Deepmind released a Deep <script type="math/tex">Q</script> Learning reinforcement learning based learning algorithm that was able to master several games from Atari 2600 sheerly based of the pixels in the screen of the game. In this blog, we will test out the Deep <script type="math/tex">Q</script> network on the Atari game Space Invaders, using OpenAI Gym, incorporating a couple newer architecture changes proposed in more recent papers, Dueling Deep <script type="math/tex">Q</script> networks (<strong>DQN</strong>) and Double Deep <script type="math/tex">Q</script> networks (<strong>DDQN</strong>).</p>

<h2 id="background">Background</h2>
<hr />
<p>Suppose that you are located somewhere in an unknown grid. At any timestep you may only move up, right, down or left. Each resulting action will give some amount of reward. Your
oal is to the find the optimal set of moves so that you will have the maximum amount of award after <script type="math/tex">T</script> timesteps. Doesn’t sound that bad? Sure, but what if you were only given a limited number trials to explore moves? Furthermore, what if the rewards were very sparse? Perhaps you will only start getting rewards after the 20th move despite the fact that it was your second move that was crucial for you get a reward.</p>

<p>The above situation is exactly an example of a problem in reinforcement learning. In reinforcement learning, we are given a set of possible states and actions. We assume that a state will have the same set of actions regardless of our moves previously to get to the state. We then wish
to find the optimum behavior such that some reward is maximized. We define  <script type="math/tex">V_s</script> given a state <script type="math/tex">s</script> to be equal to the amount of total award
we can get from state <script type="math/tex">s</script> assuming optimal movement. That is</p>

<script type="math/tex; mode=display">\begin{align*}
V_s = \max \sum_{t=0}^T R_t
\end{align*}</script>

<p>where <script type="math/tex">R_0</script> denotes the amount of award received in the first timestep and so forth.</p>

<p>Note however that the above equation does not generalize when we could potentially play forever. If we allow <script type="math/tex">T</script> to approach infinity, then the sum will always approach infinity. However, we wish to differentiate between all strategies that are infinitely long. To deal with this problem we introduce a 
discount factor <script type="math/tex">\gamma</script> and multiply the reward we get at each future timestep by multiplicative factors of <script type="math/tex">\gamma</script>. As we view results much</p>

<script type="math/tex; mode=display">\begin{align*}
V_s = \max \sum_{t=0}^{\infty} \gamma^tR_t
\end{align*}</script>

<p>further in the future, their value will then be exponentially decreased by <script type="math/tex">\gamma</script>, effectively minimizing rewards in the far future. This can also be justified by decreased
confidence in the future.</p>

<h3 id="q-value"><script type="math/tex">Q</script> Value</h3>
<hr />
<p>An alternative method to assigning values to states is to assign values to state, action pairs instead of just states. Specifically, we define <script type="math/tex">Q(s,a)</script> to be equal to the total amount of discounted reward that we can get if we are initially in states <script type="math/tex">s</script> and do action <script type="math/tex">a</script>. We refer to <script type="math/tex">Q(s,a)</script> for an action <script type="math/tex">a</script> as the <script type="math/tex">Q</script> value of the action. Assume doing some action a leads to subsequent state <script type="math/tex">s'</script>, then note that we have the following:</p>

<script type="math/tex; mode=display">\begin{align*}
Q(s,a) = r(s,a)+\gamma \max_a Q(s', a)
\end{align*}</script>

<p>where <script type="math/tex">r(s,a)</script> is the reward received from being in state <script type="math/tex">s</script> and doing action <script type="math/tex">a</script>. The above equation is true since when we reach state <script type="math/tex">s'</script>, the action with the maximum <script type="math/tex">Q</script> value will
be the optimal to take to maximize reward.</p>

<p>In normal reinforcement learning under <script type="math/tex">Q</script> learning, we wish to calculate the value of <script type="math/tex">Q(s, a)</script> for all values of s and a. Deep reinforcement learning is pretty similar, except that our state consists
of the pixel values of the screen. This allows our reinforcement learning algorithm to easy generalize to any game that can be displayed on a screen. Unfortunately, if we were to try to explicitly construct a table for all possible values of s and a, it would absolutely gigantic. If we take our state as the last 3 frames in a game with screen size 100 by 100, we would have over <script type="math/tex">10^{12}</script> possible states!</p>

<p>Fortunately though, if we are using pixels as the possible states that we are in, there is likely a lot of repetition in our states where two very similar states are likely to have the same <script type="math/tex">Q</script> value. Neural networks are very good at learning functions on these repetitive states. In repetitive image processing, convolutional neural networks(CNN) are the method of choice.  Therefore, we construct a CNN whose input is the state of the last couple frames of the game and whose output layer is the estimated <script type="math/tex">Q</script> values for the list of possible actions. A tutorial on CNN can be found <a href="http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/">here</a></p>

<h2 id="deep-q-learning">Deep <script type="math/tex">Q</script> Learning</h2>
<hr />

<p>As mentioned above in deep <script type="math/tex">Q</script> learning, we construct a CNN to process input states and output <script type="math/tex">Q</script> values for possible actions. For the particular game of space invaders
, we construct a  network of the following architecture.</p>

<table class="table-striped mbtablestyle pagination-centered center">
  <thead>
    <tr>
      <th><strong>Name</strong></th>
      <th style="text-align: left"><strong>Input Shape</strong></th>
      <th style="text-align: center"><strong>Filter Size</strong></th>
      <th style="text-align: center"><strong>Filter Number</strong></th>
      <th style="text-align: center"><strong>Stride</strong></th>
      <th style="text-align: center"><strong>Output Shape</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>conv1</td>
      <td style="text-align: left">3x84x84</td>
      <td style="text-align: center">8x8</td>
      <td style="text-align: center">32</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">32x20x20</td>
    </tr>
    <tr>
      <td>conv2</td>
      <td style="text-align: left">32x20x20</td>
      <td style="text-align: center">4x4</td>
      <td style="text-align: center">64</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">64x9x9</td>
    </tr>
    <tr>
      <td>conv3</td>
      <td style="text-align: left">64x9x9</td>
      <td style="text-align: center">3x3</td>
      <td style="text-align: center">64</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">64x7x7</td>
    </tr>
    <tr>
      <td>flatten</td>
      <td style="text-align: left">64x7x7</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">512</td>
    </tr>
    <tr>
      <td>fc1</td>
      <td style="text-align: left">512</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">6</td>
    </tr>
  </tbody>
</table>

<p>where we set the final output layer to be equal to 6 since this is the number of actions our agent is allowed to move.</p>

<p>We can implement the CNN in Keras using the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">construct_q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Convolution2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="n">NUM_FRAMES</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'relu'</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Convolution2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'relu'</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Convolution2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'relu'</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'relu'</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">))</span>
</code></pre></div></div>

<p>One interesting thing to note is that we are missing a max pooling layer that is usually found in CNNs for vision detection. This is because
max pooling is primarily used to implement translational invariance – which we don’t really care about in a convolution neural network.</p>

<p>Another thing to note is that our CNN takes as input a image of size 84x84. This is different then the default resolution of the Atari gamescreen of
192 by 160. This is to make training computationally easier. Furthermore, we convert each input image of 3 RGB channels to 1 black white channel since color doesn’t really 
effect gameplay. Finally, for each input image into our CNN, we stack the last three frames that have occured – this way we can sense if a bullet is 
falling down.</p>

<p>The code for converting the last three frames to one single channel image is below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">convert_process_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""Converts the list of NUM_FRAMES images in the process buffer
        into one training sample"""</span>
        <span class="n">black_buffer</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_RGB2GRAY</span><span class="p">),</span> <span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">90</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_buffer</span><span class="p">)</span>
        <span class="n">black_buffer</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">85</span><span class="p">,</span> <span class="p">:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">black_buffer</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">black_buffer</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>In the above code,  <em>self.process_buffer</em> contains the last three full sized 192x160x3 pictures.</p>

<h3 id="replay-buffer">Replay Buffer</h3>
<hr />

<p>A problem when we are training our network is the fact that if we only train on frames of data as they come in, we would be overfitting
on the last few frames of the data. As a result, we keep a buffer of all the last 20000 experiences we have experienced so far and
randomly sample a batch of 64 images to learn on at each step of the game. This way, we won’t overfit on the most recent frames. This is
called <strong>experience replay</strong>.</p>

<p><strong>Experience replay</strong> is also found biologically!  Studies in rats have shown that replay of events is vital for rats to learn tasks. Specifically, they
found that selective emphasis of replay of surprising events in the past.  One possible incorporation of this fact into our network is
to replay past events, but with increased emphasis on events with high temporal difference(events that are very different than
what we expect) as done in the paper <a href="https://arxiv.org/abs/1511.05952">here</a>
We use the following data structure for our replay buffer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">:</span>
    <span class="s">"""Constructs a buffer object that stores the past moves
    and samples a set of subsamples"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer_size</span> <span class="o">=</span> <span class="n">buffer_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="nb">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">s2</span><span class="p">):</span>
        <span class="s">"""Add an experience to the buffer"""</span>
        <span class="c1"># S represents current state, a is action,
</span>        <span class="c1"># r is reward, d is whether it is the end, 
</span>        <span class="c1"># and s2 is next state
</span>        <span class="n">experience</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">s2</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_size</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="nb">buffer</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="s">"""Samples a total of elements equal to batch_size from buffer
        if buffer contains enough elements. Otherwise return all elements"""</span>

        <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="nb">buffer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="nb">buffer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># Maps each experience in batch in batches of states, actions, rewards
</span>        <span class="c1"># and new states
</span>        <span class="n">s_batch</span><span class="p">,</span> <span class="n">a_batch</span><span class="p">,</span> <span class="n">r_batch</span><span class="p">,</span> <span class="n">d_batch</span><span class="p">,</span> <span class="n">s2_batch</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">s_batch</span><span class="p">,</span> <span class="n">a_batch</span><span class="p">,</span> <span class="n">r_batch</span><span class="p">,</span> <span class="n">d_batch</span><span class="p">,</span> <span class="n">s2_batch</span>
</code></pre></div></div>

<h3 id="exploration">Exploration</h3>
<hr />

<p>How do we play the game while we are training our network? Do we continuously choose what we believe is the best action? But if we do this
, how will we be able to discover a new move? This general problem is known and <strong>exploration vs exploitation tradeoff</strong>.  We remedy this problem, we use an <script type="math/tex">\epsilon</script> exploration policy to play the game. This means that with probability <script type="math/tex">\epsilon</script>
you do a random action. Otherwise, we select the action with the highest <script type="math/tex">Q</script> value(what we believe is the best action) from our current state. In addition,
since at the beginning of training, our belief in <script type="math/tex">Q</script> values is completely baseless, we slowly linearly decrease our <script type="math/tex">\epsilon</script> from <script type="math/tex">1</script> to <script type="math/tex">0.1</script>. We can do this in code as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">predict_movement</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
        <span class="s">"""Predict movement of game controler where is epsilon
        probability randomly move."""</span>
        <span class="n">q_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="n">NUM_FRAMES</span><span class="p">),</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">opt_policy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_actions</span><span class="p">)</span>
        <span class="n">rand_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">rand_val</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">opt_policy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">NUM_ACTIONS</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">opt_policy</span><span class="p">,</span> <span class="n">q_actions</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">opt_policy</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="loss-function-and-target-networks">Loss Function and Target Networks</h3>
<hr />

<p>As mentioned above, we wish that each of the outputs of our CNN be equal to the <script type="math/tex">Q</script> value of
a respective action. We know from this fact that outputs should satisfy that</p>

<script type="math/tex; mode=display">\begin{align*}
Q(s,a) = r+\gamma \max_a Q(s', a)
\end{align*}</script>

<p>Therefore, for every <script type="math/tex">(s, a, r, s')</script> action tuple in our replay buffer we minimize the discrepancy between the <script type="math/tex">Q</script> value predicted directly from the neural network and
the <script type="math/tex">Q</script> value constructed from the subsequent reward and maximum <script type="math/tex">Q</script> value of the resultant state, if the state is non-terminal. If the state is terminal, then the expect <script type="math/tex">Q</script> value should just be the reward. I used MSE(mean square error) as a loss function. This can be implemented in Keras below</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_batch</span><span class="p">,</span> <span class="n">a_batch</span><span class="p">,</span> <span class="n">r_batch</span><span class="p">,</span> <span class="n">d_batch</span><span class="p">,</span> <span class="n">s2_batch</span><span class="p">,</span> <span class="n">observation_num</span><span class="p">):</span>
        <span class="s">"""Trains network to fit given parameters"""</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">s_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">NUM_ACTIONS</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="n">NUM_FRAMES</span><span class="p">),</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">fut_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s2_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="n">NUM_FRAMES</span><span class="p">),</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">a_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">r_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">d_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
                <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">a_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">DECAY_RATE</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">fut_action</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train_on_batch</span><span class="p">(</span><span class="n">s_batch</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
</code></pre></div></div>

<p>Note that in the above code, we actually use a second “target” network to predict the
Q values of the transitioned state <script type="math/tex">s'</script>. The second “target” network is set to the weights of the original network every so many frames but is otherwise unchanged. This allows the deep <script type="math/tex">Q</script> network to converge more quickly, since otherwise we could enter a self
feedback loop where we continuously estimate higher and higher <script type="math/tex">Q</script> values. The code for
setting the weights of the target network is below</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">target_train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">model_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">model_weights</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="results-for-double-deep-q-network">Results for Double Deep <script type="math/tex">Q</script> Network</h2>
<hr />

<p>Combining all the code above we construct a Double Deep <script type="math/tex">Q</script> Networks(the target network is our “second” network) , after training for about 1,000,000 frames, we get an average score of around
260 on the game of space invaders(full code can be found
<a href="https://github.com/yilundu/DQN-DDQN-on-Space-Invaders">here</a>). Below is a video of the Double Deep <script type="math/tex">Q</script> Network playing.</p>

<p><img src="http://localhost:4000/images/output.gif" alt="Picture space invaders" class="img-responsive center" /></p>

<p>We can visualize <script type="math/tex">Q</script> values of different actions as we train the DDQN, which is shown in the plot below. Loosely, these Q values can represent how our network is learning
how to play this game.</p>

<p><img src="http://localhost:4000/images/q-loss-graph.png" alt="Picture Average $$Q$$ Loss" class="img-responsive center" /></p>

<h2 id="dueling-q-network">Dueling <script type="math/tex">Q</script> Network</h2>
<hr />

<p>One problem with our above implementation of a Deep <script type="math/tex">Q</script> Network is that we are currently directly estimating the value of being at a state and executing a specific action. However, much of the time, the value of doing any action doesn’t really influence the value of being at a specific state. In a dueling <script type="math/tex">Q</script> network architecture, we seek to separate</p>

<script type="math/tex; mode=display">\begin{align}
Q(s,a) = A(s,a) + V(s)
\end{align}</script>

<p>where under this definition, <script type="math/tex">A(s,a)</script> will represents the advantage of making a certain action and <script type="math/tex">V(s)</script> will represents the current value of being at a certain state.</p>

<p>How do we do this? We construct two separate two streams in our CNN - one to estimate
the value of a state and another to estimate the advantage of each of the actions.
Note that both streams share the same weights for the convolutional layers. We then combine these layers to predict the <script type="math/tex">Q</script> values of each action. Unfortunately, directly summing these separate streams gives us no guarantees that the first stream will actually predict the value of a state. Instead we combine them with criterion</p>

<script type="math/tex; mode=display">\begin{align}
Q(s,a) = V(s) + A(s,a) - \frac{1}{\|a\|}\sum_{a'}A(s,a')
\end{align}</script>

<!---_ -->

<p>Under the above criterion, on average, the advantage stream will be on average equal to 0, leading the value stream to approximately predict the value of the state.</p>

<p>We can implement this in Keras using the following code</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">construct_q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Uses the network architecture found in DeepMind paper
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
        <span class="n">input_layer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="n">NUM_FRAMES</span><span class="p">))</span>
        <span class="n">conv1</span> <span class="o">=</span> <span class="n">Convolution2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
        <span class="n">conv2</span> <span class="o">=</span> <span class="n">Convolution2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">conv1</span><span class="p">)</span>
        <span class="n">conv3</span> <span class="o">=</span> <span class="n">Convolution2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">)(</span><span class="n">conv2</span><span class="p">)</span>
        <span class="n">flatten</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">conv3</span><span class="p">)</span>
        <span class="n">fc1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">)(</span><span class="n">flatten</span><span class="p">)</span>
        <span class="n">advantage</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">)(</span><span class="n">fc1</span><span class="p">)</span>
        <span class="n">fc2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">)(</span><span class="n">flatten</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">fc2</span><span class="p">)</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="n">merge</span><span class="p">([</span><span class="n">advantage</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="n">mode</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">+</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">,))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">input_layer</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="p">[</span><span class="n">policy</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="results-for-duel-q-network">Results for Duel <script type="math/tex">Q</script> Network</h2>
<hr />

<p>Under the Duel <script type="math/tex">Q</script> Network, after about 600,000 frames of training, I got an average score of  297, better than the DDQN. Interestingly, the movement is also very
different then the Deep <script type="math/tex">Q</script> Network(I tried several different initializations of both networks and seemed to always result in these distinctive behaviors of play)!</p>

<p><img src="http://localhost:4000/images/duelq.gif" alt="Picture space invaders" class="img-responsive center" /></p>

<h2 id="conclusion">Conclusion</h2>
<hr />
<p>In the above blog post, we explored how we can use Deep <script type="math/tex">Q</script> Networks to achieve decent performance on the Space Invaders game. Given additional training time, the performance on the tasks will probably increase.</p>

<p>As a final word, there are many possible improvements we can make on the architecture described. One interesting possibility, inspired partly from cognitive science,
is prioritized replay, where we seek to preferentially replay events that are very ‘different’(our estimated <script type="math/tex">Q</script> values significantly different than actual <script type="math/tex">Q</script> values)
from what we expect. Recently, Google DeepMind released Asynchronous Advantage Actor Critic (A3C) method which generally performs better than variants of DQN on almost all games in the Atari suite, including Space Invaders.</p>

<h3 id="acknowledgments">Acknowledgments</h3>
<hr />
<p>This blog post would not be possible without so many amazing resources online! Code from <a href="http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html">here</a> and <a href="https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html">here</a> were used as references for the code presented above.</p>

<p><em>There were a lot of parts of code in the above implementation and if there were any errors please let me know!</em></p>

  </div>

  <div class="date">
    Written on December 24, 2016
  </div>

  

</article>

        </div>

        <div class="wrapper-footer">
            <div class="container">
                <footer class="footer">
                    












                </footer>
            </div>
            
            <div id="disqus_thread" class="container"></div>
            <script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
            /*
               var disqus_config = function () {
               this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
               this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
               };
             */
            (function() { // DON'T EDIT BELOW THIS LINE
                var d = document, s = d.createElement('script');
                s.src = '//blog-awbymgp6mk.disqus.com/embed.js';
                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
            })();
            </script>
            <script id="dsq-count-scr" src="//blog-awbymgp6mk.disqus.com/count.js" async></script>
            <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

            
        </div>

        

    </body>
</html>
