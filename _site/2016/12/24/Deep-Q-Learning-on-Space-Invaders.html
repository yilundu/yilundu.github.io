<!DOCTYPE html>
<html>
  <head>
    <title>Deep Q Learning on Space Invaders Using Keras –  – </title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="Deep Q Learning on Space Invaders Using Keras

" />
    <meta property="og:description" content="Deep Q Learning on Space Invaders Using Keras

" />
    
    <meta name="author" content="" />

    
    <meta property="og:title" content="Deep Q Learning on Space Invaders Using Keras" />
    <meta property="twitter:title" content="Deep Q Learning on Space Invaders Using Keras" />
    


    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title=" - " href="/feed.xml" />
	<script type="text/javascript" async
	  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/"></a></h1>
            <p class="site-description"></p>
          </div>

          <nav>
            <a href="/">Blog</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Deep Q Learning on Space Invaders Using Keras</h1>

  <div class="entry">
    <p>Deep Q Learning on Space Invaders Using Keras</p>

<p>Over the winter break I thought it would be fun to experiment with deep reinforcement learning. Using the ideas of reinforcement learning computers have been able to do amazing things such master the gane of <a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf">Go</a>, play 3D racing games competitvely, and undergo complex manipulations of the environment around them that completely defy explicit programming!</p>

<p>A little under 3 years ago, Deepmind released a Deep Q Learning reinforcement learning based learning algorithm that was able to master several games from Atari 2600 sheerly based of the pixels in the screen of the game. In this blog, we will test out the Deep Q network on the Atari game Space Invaders, encoperating a couple newer architecture changes proposed in more recent papers, Dueling Deep Q networks(DQN) and Double Deep Q networks(DDQN).</p>

<h1 id="background">Background</h1>
<p>Suppose that you are located somewhere in an unknown grid. At any timestep you may only move up, right, down or left. Each resulting action will give some amount of reward. Your
goal is to the find the optimal set of moves so that you will have the maximum amount of award after <i>T</i> timesteps. Doesn’t sound that bad? Sure, but what if you were only given a limited number trials to explore moves? Furthermore, what if the rewards were very sparse? Perhaps you will only start getting rewards after the 20th move despite the fact that it was your second move that was crucial for you get a reward.</p>

<p>The above siutation is exactly an example of a problem in reinforcement learning. In reinforcement learning, we are given a set of possible states and actions. We assume that a state will have the same set of actions regardless of our moves previosuly to get to the state. We then wish
to find the optimum behavior such that some reward is maximized. We define  <script type="math/tex">V_s</script> given a state <script type="math/tex">s</script> to be equal to the amount of total award
we can get from state <script type="math/tex">s</script> assuming optimal movement. That is
<script type="math/tex">\begin{align*}
V_s = \max \sum_{t=0}^T R_t
\end{align*}</script>
where $R_0$ denotes the amount of award received in the first timestep and so forth.</p>

<p>Note however that the above equation does not generalize when we could potentially play forever. If we allow <script type="math/tex">T</script> to approach infinity, then the sum will always approach infinity. However, we wish to differentiate between all strategies that are infinitely long. To deal with this problem we introduce a 
discount factor <script type="math/tex">\gamma</script> and multiply the reward we get at each future timestep by <script type="math/tex">\gamma</script>. We then have that 
<script type="math/tex">\begin{align*}
V_s = \max \sum_{t_0}^{\infnty} \gamma^tR_t
\end{align*}</script>
The discount factor decreases the value of rewards in as the future which can also be interpreted as decreased certaintity in the reward the future.</p>

<h1 id="q-value">Q Value</h1>

<p>An alternative method to assigning values to states is to assign values to state, action pairs instead of just states. Specifically, we define <script type="math/tex">Q(s,a)</script> to be equal to the total amount of discounted reward that we can get if were in initially states s and did action a. Assume to doing some action a leads to subsequent state s’, then note that we have that:</p>

<script type="math/tex; mode=display">\begin{align*}
Q(s,a) = r+\gamma \max_a Q(s', a)
\end{align*}</script>

<p>where r is the reward received from being in state s and doing action a. The above equation is true since when we reach state s’, the action with the maximum Q value will
be the optimal to take to maximize reward.</p>

<p>In normal reinforcement learning under Q learning, we wish to calculate the value of <script type="math/tex">Q(s, a)</script> for all values of s and a. In deep reinforcement learning, we take as our state
the values of pixels on the screen. If we were to try to explicitly construct a table for all possible values of s and a, it would absolutely gigantic. If we take our state as the last 3 frames in a game with screen size 100 by 100, we would have over <script type="math/tex">10^12</script> states.</p>

<p>Given that we are using pixels as the states there is likely a lot of reptition in our states where two very similar states are likely to have the same Q value.
This is exactly what convolutional neural networks(CNN) are good at! Therefore, we construct a CNN to whose input is a state of the last couple frames in a game and whose output layer is the estimated Q values for the list of possible actions.</p>

<h1 id="code">Code</h1>

<p>Enough explanation.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">hello_word</span><span class="p">():</span>
    <span class="k">print</span> <span class="s">"Hello"</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></figure>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">hello</span><span class="p">():</span>
    <span class="k">print</span> <span class="s">"Yolanda"</span>
</code></pre>
</div>

<h1 id="conclusion">Conclusion</h1>
<p>In the above blog post, we explored how we can use Deep Q Networks to achieve decent performance on the Space Invaders game. Given additional training time, the performance on the tasks will likely. 
As a final word, there many possible improvements we can make on the architecture described. One interesting possibility, inspired partly from cognitive science
is prioritized replay, where we seek to preferentially replay events that are very ‘different’(our estimated Q values significantly different than actual Q values)
from what we expect. Recently, Google DeepMind released Asynchronous Advantage Actor Critic (A3C) method which generally performs better than variants of DQN on almost all games in the Atari suite, including Space Invaders.</p>


  </div>

  <div class="date">
    Written on December 24, 2016
  </div>

  

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          












        </footer>
      </div>
    </div>

    
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//blog-awbymgp6mk.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<script id="dsq-count-scr" src="//blog-awbymgp6mk.disqus.com/count.js" async></script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                                
    
    

  </body>
</html>
